{"nbformat_minor": 0, "nbformat": 4, "cells": [{"cell_type": "code", "execution_count": 3, "metadata": {"collapsed": false}, "outputs": [{"output_type": "error", "evalue": "File C:\\Users\u0007x28957\\Documents\\BigData\u0002.Exercises\\Kagle_titanic\\Data\train.csv does not exist", "ename": "IOError", "traceback": ["\u001b[0;31m\u001b[0m", "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)", "\u001b[0;32m<ipython-input-3-8a07bffc6524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C:\\Users\\ax28957\\Documents\\BigData\\02.Exercises\\Kagle_titanic\\Data\\train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m/home/nbuser/anaconda2_20/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/nbuser/anaconda2_20/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/nbuser/anaconda2_20/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/nbuser/anaconda2_20/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/nbuser/anaconda2_20/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n", "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n", "\u001b[0;31mIOError\u001b[0m: File C:\\Users\u0007x28957\\Documents\\BigData\u0002.Exercises\\Kagle_titanic\\Data\train.csv does not exist"]}], "source": "# Data analysis and wrangling\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n#load data\ntrain= pd.read_csv(\"C:\\Users\\ax28957\\Documents\\BigData\\02.Exercises\\Kagle_titanic\\Data\\train.csv\")"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": "\ntest = pd.read_csv(os.path.join('data', 'test.csv'))\n\ntrain.info()\ntrain.head()"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": "#number of survived\ntrain['Survived'].value_counts(normalize=True)\nsns.countplot(train['Survived'])\n\n#survuved by class\ntrain['Survived'].groupby(train['Pclass']).mean()\nsns.countplot(train['Pclass'], hue=train['Survived'])\n\n#Split by Name Title\n#lambda anonymous function \ntrain['Name'].head()\ntrain['Name_Title'] = train['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])\ntrain['Name_Title'].value_counts()\n\n\n#Grouping by Name Title\n#pd.qcut(x, q, labels=None, retbins=False, precision=3) - quantile based on discretization function\ntrain['Survived'].groupby(train['Name_Title']).mean()\n\n#Long Names\ntrain['Name_Len'] = train['Name'].apply(lambda x: len(x))\ntrain['Survived'].groupby(pd.qcut(train['Name_Len'],5)).mean()\n\npd.qcut(train['Name_Len'],5).value_counts()\n\n#divided by Sex -  total passengers\ntrain['Sex'].value_counts(normalize=True)\n\n#divided by sex survived\ntrain['Survived'].groupby(train['Sex']).mean()\n\n\n#survived by age, verify the % that has this data\ntrain['Survived'].groupby(train['Age'].isnull()).mean()\n\n#survived by age\ntrain['Survived'].groupby(pd.qcut(train['Age'],5)).mean()\n\npd.qcut(train['Age'],5).value_counts()\n\n#sibssp\ntrain['Survived'].groupby(train['SibSp']).mean()\ntrain['SibSp'].value_counts()\n\n#ticket\ntrain['Ticket'].head(n=10)\ntrain['Ticket_Len'] = train['Ticket'].apply(lambda x: len(x))\ntrain.groupby(['Ticket_Len'])['Survived'].mean()\ntrain['Ticket_Len'].value_counts()\n\n#ticket first letter\ntrain['Ticket_Lett'] = train['Ticket'].apply(lambda x: str(x)[0])\ntrain['Ticket_Lett'].value_counts()\ntrain.groupby(['Ticket_Lett'])['Survived'].mean()\n\n#Fare ticket\npd.qcut(train['Fare'], 3).value_counts()\ntrain['Survived'].groupby(pd.qcut(train['Fare'], 3)).mean()\n\n#Relation between Class and fare\npd.crosstab(pd.qcut(train['Fare'], 5), columns=train['Pclass'])\n\n#cabin leter\ntrain['Cabin_Letter'] = train['Cabin'].apply(lambda x: str(x)[0])\ntrain['Cabin_Letter'].value_counts()\ntrain['Survived'].groupby(train['Cabin_Letter']).mean()\n\n#cabin number\ntrain['Cabin_num'] = train['Cabin'].apply(lambda x: str(x).split(' ')[-1][1:])\ntrain['Cabin_num'].replace('an', np.NaN, inplace = True)\ntrain['Cabin_num'] = train['Cabin_num'].apply(lambda x: int(x) if not pd.isnull(x) and x <> '' else np.NaN)\n\npd.qcut(train['Cabin_num'],3).value_counts()\ntrain['Survived'].groupby(pd.qcut(train['Cabin_num'], 3)).mean()\n\n#correlation between survived and cabin number\ntrain['Survived'].groupby(pd.qcut(train['Cabin_num'], 3)).mean()\n\n#embarked\ntrain['Embarked'].value_counts()\ntrain['Embarked'].value_counts(normalize=True)\ntrain['Survived'].groupby(train['Embarked']).mean()\nsns.countplot(train['Embarked'], hue=train['Pclass'])\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": "# >> Modelling\n#Creation of two columns: Lenght name and title name\ndef names(train, test):\n    for i in [train, test]:\n        i['Name_Len'] = i['Name'].apply(lambda x: len(x))\n        i['Name_Title'] = i['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])\n        del i['Name']\n    return train, test\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": "#null values of age filling by average of pessengers by title and class\ndef age_impute(train, test):\n    for i in [train, test]:\n        i['Age_Null_Flag'] = i['Age'].apply(lambda x: 1 if pd.isnull(x) else 0)\n    train['mean'] = train.groupby(['Name_Title', 'Pclass'])['Age'].transform('mean')\n    train['Age'] = train['Age'].fillna(train['mean'])\n    z = test.merge(train, on=['Name_Title', 'Pclass'], how='left').drop_duplicates(['PassengerId_x'])\n    test['Age'] = np.where(test['Age'].isnull(), z['mean'], test['Age'])\n    test['Age'] = test['Age'].fillna(test['Age'].mean())\n    del train['mean']\n    return train, test\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": "#combine the SibSp and Parch columns into a new variable that indicates family size, and group the family size variable into three categories.\ndef fam_size(train, test):\n    for i in [train, test]:\n        i['Fam_Size'] = np.where((i['SibSp']+i['Parch']) == 0 , 'Solo',\n                           np.where((i['SibSp']+i['Parch']) <= 3,'Nuclear', 'Big'))\n        del i['SibSp']\n        del i['Parch']\n    return train, test\n\n#ticket length\ndef ticket_grouped(train, test):\n    for i in [train, test]:\n        i['Ticket_Len'] = i['Ticket'].apply(lambda x: len(x))\n        del i['Ticket']\n    return train, test\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": "#extraction of first letter of cabin and it number\n\ndef cabin(train, test):\n    for i in [train, test]:\n        i['Cabin_Letter'] = i['Cabin'].apply(lambda x: str(x)[0])\n        del i['Cabin']\n    return train, test\n\n\ndef cabin_num(train, test):\n    for i in [train, test]:\n        i['Cabin_num1'] = i['Cabin'].apply(lambda x: str(x).split(' ')[-1][1:])\n        i['Cabin_num1'].replace('an', np.NaN, inplace = True)\n        i['Cabin_num1'] = i['Cabin_num1'].apply(lambda x: int(x) if not pd.isnull(x) and x <> '' else np.NaN)\n        i['Cabin_num'] = pd.qcut(train['Cabin_num1'],3)\n    train = pd.concat((train, pd.get_dummies(train['Cabin_num'], prefix = 'Cabin_num')), axis = 1)\n    test = pd.concat((test, pd.get_dummies(test['Cabin_num'], prefix = 'Cabin_num')), axis = 1)\n    del train['Cabin_num']\n    del test['Cabin_num']\n    del train['Cabin_num1']\n    del test['Cabin_num1']\n    return train, test\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": "#fill the embarket data with the most common data : 'S'\ndef embarked_impute(train, test):\n    for i in [train, test]:\n        i['Embarked'] = i['Embarked'].fillna('S')\n    return train, test\n\n#fill the fare data with the average\ntest['Fare'].fillna(train['Fare'].mean(), inplace = True)\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": "#convertion of our categorical columns into dummyvariables.\n\ndef dummies(train, test, columns = ['Pclass', 'Sex', 'Embarked', 'Ticket_Lett', 'Cabin_Letter', 'Name_Title', 'Fam_Size']):\n    for column in columns:\n        train[column] = train[column].apply(lambda x: str(x))\n        test[column] = test[column].apply(lambda x: str(x))\n        good_cols = [column+'_'+i for i in train[column].unique() if i in test[column].unique()]\n        train = pd.concat((train, pd.get_dummies(train[column], prefix = column)[good_cols]), axis = 1)\n        test = pd.concat((test, pd.get_dummies(test[column], prefix = column)[good_cols]), axis = 1)\n        del train[column]\n        del test[column]\n    return train, test"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": "#drops any columns that haven't already been dropped\ndef drop(train, test, bye = ['PassengerId']):\n    for i in [train, test]:\n        for z in bye:\n            del i[z]\n    return train, test"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": "# execute function in order to build a dataset\ntrain = pd.read_csv(os.path.join('data', 'train.csv'))\ntest = pd.read_csv(os.path.join('data', 'test.csv'))\ntrain, test = names(train, test)\ntrain, test = age_impute(train, test)\ntrain, test = cabin_num(train, test)\ntrain, test = cabin(train, test)\ntrain, test = embarked_impute(train, test)\ntrain, test = fam_size(train, test)\ntest['Fare'].fillna(train['Fare'].mean(), inplace = True)\ntrain, test = ticket_grouped(train, test)\ntrain, test = dummies(train, test, columns = ['Pclass', 'Sex', 'Embarked', \n                                              'Cabin_Letter', 'Name_Title', 'Fam_Size'])\ntrain, test = drop(train, test)"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": "#check columns on dataset\nlen(train.columns)"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": "#Hyperparameter Tuning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_features='auto',\n                                oob_score=True,\n                                random_state=1,\n                                n_jobs=-1)\n\nparam_grid = { \"criterion\"   : [\"gini\", \"entropy\"],\n             \"min_samples_leaf\" : [1, 5, 10],\n             \"min_samples_split\" : [2, 4, 10, 12, 16],\n             \"n_estimators\": [50, 100, 400, 700, 1000]}\n\ngs = GridSearchCV(estimator=rf,\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  cv=3,\n                  n_jobs=-1)\n\ngs = gs.fit(train.iloc[:, 1:], train.iloc[:, 0])\n\n\nprint(gs.best_score_)\nprint(gs.best_params_)\n#print(gs.cv_results_)"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": "# model estimation and evaluation\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(criterion='entropy', \n                             n_estimators=50,\n                             min_samples_split=16,\n                             min_samples_leaf=1,\n                             max_features='auto',\n                             oob_score=True,\n                             random_state=1,\n                             n_jobs=-1)\nrf.fit(train.iloc[:, 1:], train.iloc[:, 0])\nprint \"%.4f\" % rf.oob_score_"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": "#variable importance\npd.concat((pd.DataFrame(train.iloc[:, 1:].columns, columns = ['variable']), \n           pd.DataFrame(rf.feature_importances_, columns = ['importance'])), \n          axis = 1).sort_values(by='importance', ascending = False)[:20]"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": "#predict the target variable \nnp.shape(test)\n\n\npredictions = rf.predict(test)\npredictions = pd.DataFrame(predictions, columns=['Survived'])\ntest = pd.read_csv(os.path.join('data', 'test.csv'))\npredictions = pd.concat((test.iloc[:, 0], predictions), axis = 1)\npredictions.to_csv(os.path.join('submission_files', 'y_test.csv'), sep=\",\", index = False)\n\n"}], "metadata": {"kernelspec": {"name": "python2", "language": "python", "display_name": "Python 2"}, "language_info": {"file_extension": ".py", "mimetype": "text/x-python", "version": "2.7.13", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 2}, "name": "python", "pygments_lexer": "ipython2"}}}